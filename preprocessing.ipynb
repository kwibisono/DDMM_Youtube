{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from numpy import save, load\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(x):\n",
    "    # Lemmatize the words\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmatized_list = []\n",
    "    for i in x.split():\n",
    "        lemmatized_word = lem.lemmatize(i)\n",
    "        if len(lemmatized_word)>2:\n",
    "            lemmatized_list.append(lemmatized_word)\n",
    "    return lemmatized_list\n",
    "\n",
    "\n",
    "def remove_stopwords(x, all_stopwords):\n",
    "    # Remove stopwords\n",
    "    x = [i for i in x if i not in all_stopwords]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bow_file(df, version):\n",
    "    # Define stopwords\n",
    "    youtube_stopwords = ['watch', 'watching', 'previous', 'like', 'subscribe', 'official', 'website', \\\n",
    "                     'newletter', 'channel', 'video', 'videos', 'youtube', 'subscribe'\\\n",
    "                     'facebook', 'instagram', 'twitter', 'snapchat', 'reddit', 'here',\\\n",
    "                     'join', 'follow', 'tweet', 'email', 'check', 'find',\\\n",
    "                     'sponsor', 'sponsored', 'support', 'supported', 'music']\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "    all_stopwords = []\n",
    "    all_stopwords.extend(youtube_stopwords)\n",
    "    all_stopwords.extend(nltk_stopwords)\n",
    "    all_stopwords = set(all_stopwords)\n",
    "    \n",
    "    # Lemmatize stopwords\n",
    "    lem = WordNetLemmatizer()\n",
    "    all_stopwords = [lem.lemmatize(i) for i in all_stopwords]\n",
    "    \n",
    "    print('Combining columns...')\n",
    "    # Combine columns that correspond to the defined version\n",
    "    df['words'] = df[version].apply(lambda x: ' '.join(x), axis=1)\n",
    "    \n",
    "    print('Removing URLs...')\n",
    "    # Remove URLs\n",
    "    df['words'] = df['words'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "    \n",
    "    print('Removing newline symbols...')\n",
    "    # Remove newline symbol\n",
    "    df['words'] = df['words'].apply(lambda x: x.replace('\\\\n', ''))\n",
    "    \n",
    "    print('Removing punctuations, numbers, and converting to lower case...')\n",
    "    # Remove punctuation and numbers, and convert to lower case\n",
    "    df['words'] = df['words'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', x).lower())\n",
    "    \n",
    "    print('Lemmatizing the words...')\n",
    "    # Lemmatize the words (each row input is a non-tokenized sentence)\n",
    "    df['words'] = df['words'].apply(lemmatize_words)\n",
    "    \n",
    "    print('Removing stopwords...')\n",
    "    # Remove stopwords\n",
    "    df['words'] = df['words'].apply(lambda x: remove_stopwords(x, all_stopwords))\n",
    "    \n",
    "    # Recombine all words\n",
    "    df['words'] = df['words'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Create Bag of Words\n",
    "    print('Creating Bag of Words...')\n",
    "    vectorizer = CountVectorizer(max_features=2500)\n",
    "    X = vectorizer.fit_transform(df['words'])\n",
    "    \n",
    "    index_word_dict = {}\n",
    "    # Create index to word dictionary\n",
    "    for i, word in enumerate(vectorizer.get_feature_names()):\n",
    "        index_word_dict[i] = word\n",
    "    \n",
    "    bow = X.toarray()\n",
    "    no_words_index = np.where(np.sum(bow == 0, axis=1) == bow.shape[1])[0]\n",
    "    bow = np.delete(bow, no_words_index, axis=0)\n",
    "    \n",
    "    return index_word_dict, bow, no_words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_meta_file(df, category_dict):\n",
    "    print('Preprocessing meta file...')\n",
    "    # Get dictionary correspondence between category_id and genre name\n",
    "    id_to_category_dict = {}\n",
    "    for item in category_dict['items']:\n",
    "        id_to_category_dict[int(item['id'])] = item['snippet']['title']\n",
    "        \n",
    "    # Apply the dictionary to the video dataframe category_id's, to get the corresponding genre name for all rows\n",
    "    df['genre'] = df['category_id'].map(id_to_category_dict)\n",
    "    \n",
    "    # Get dictionary correspondence between trending_date and t_day/t_week\n",
    "    date_to_tday_dict = {}\n",
    "    date_to_tweek_dict = {}\n",
    "    week_ind = 0\n",
    "    for i, date in enumerate(np.sort(np.unique(df['trending_date']))):\n",
    "        # t_day correspondence\n",
    "        date_to_tday_dict[date] = int(i)\n",
    "        \n",
    "        # t_week correspondence\n",
    "        if (i!=0) & (i%7==0):\n",
    "            week_ind += 1\n",
    "        date_to_tweek_dict[date] = int(week_ind)\n",
    "        \n",
    "    # Apply the dictionary to the video dataframe trending_date's, to get the corresponding t_day/t_week for all rows\n",
    "    df['t_day'] = df['trending_date'].map(date_to_tday_dict)\n",
    "    df['t_week'] = df['trending_date'].map(date_to_tweek_dict)\n",
    "    \n",
    "    # Get dictionary correspondene between trending_date and t_month\n",
    "    date_to_tmonth_dict = {}\n",
    "    for i, year_month in enumerate(np.sort(np.unique(df['trending_date'].dt.to_period('M')))):\n",
    "        date_to_tmonth_dict[year_month] = int(i)\n",
    "        \n",
    "    # Apply the dictionary to the video dataframe trending_date's, to get the corresponding t_month for all rows\n",
    "    df['t_month'] = df['trending_date'].dt.to_period('M').map(date_to_tmonth_dict)\n",
    "    \n",
    "    return df[['trending_date', 't_day', 't_week', 't_month', 'channel_title', 'title', 'tags', 'description', 'genre']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to preprocess the Trending YouTube Video Statistics Dataset from https://www.kaggle.com/datasnaek/youtube-new\n",
    "\n",
    "INPUT\n",
    "- df: dataframe of a region's video.csv file\n",
    "- category_dict: dictionary of the corresponding region's category_id.json file\n",
    "- version: list of which columns to include in the scope of the latter experiment\n",
    "    - Could (Recommmended to) take one of the following forms:\n",
    "        - ['title'], ['tags'], ['description'], ['title', 'tags'], ['title', 'description'], ['tags', 'description'], ['title', 'tags', 'description']\n",
    "\n",
    "OUTPUT:\n",
    "- Meta File: CSV File that consists of the columns ['trending_date', 't_day', 't_week', 't_month', 'channel_title', 'title', 'tags', 'description', 'genre'] for each video\n",
    "- Bag of Words (BOW) File: Numpy File where each column represents a unique word, and each row represents the count of the appearance of words per video\n",
    "- BoW Index File: Json File which contains a dictionary of the format {index: word} from the BoW\n",
    "\n",
    "Note that each row of the output files correspond to each other, and they represent a single video\n",
    "'''\n",
    "\n",
    "def preprocess_youtube_dataset(df, category_dict, version):\n",
    "    # Remove rows with NaN values\n",
    "    \n",
    "    df = df.dropna().reset_index(drop='True')\n",
    "    \n",
    "    # Convert 'trending_date' to datetime '<M8[ns]' type\n",
    "    if df['trending_date'].dtype != '<M8[ns]':\n",
    "        df['trending_date'] = df['trending_date'].apply(lambda x: datetime.datetime.strptime(x, '%y.%d.%m'))\n",
    "    \n",
    "    # BOW File preprocessing\n",
    "    index_word_dict, bow, no_words_index = preprocess_bow_file(df, version)\n",
    "\n",
    "    # Meta File preprocessing\n",
    "    meta_df = preprocess_meta_file(df.drop(no_words_index, axis=0).reset_index(drop=True), category_dict)\n",
    "    \n",
    "    # Version naming convention\n",
    "    version_name = ''\n",
    "    for column in version:\n",
    "        version_name += str(column) + '_'\n",
    "    \n",
    "    # Save index to word dictionary to json\n",
    "    with open(version_name + 'indexword.json', \"w\") as outfile:  \n",
    "        json.dump(index_word_dict, outfile) \n",
    "    \n",
    "    # Save bag of words to array\n",
    "    save(version_name + 'bow.npy', bow)\n",
    "    \n",
    "    # Save meta dataframe to csv\n",
    "    meta_df.to_csv(version_name + 'meta.csv', index=False, line_terminator='^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read video csv and category_id json data\n",
    "df = pd.read_csv('data/USvideos.csv')\n",
    "with open('data/US_category_id.json') as f:\n",
    "    category_dict =  json.load(f)\n",
    "    \n",
    "# Set version of preprocessing\n",
    "version = ['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_youtube_dataset(df, category_dict, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
