{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from gensim.models import LdaSeqModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import DDMM class by reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define DDMM model class\n",
    "\n",
    "class DDMM():\n",
    "    \n",
    "    #Miscellaneous function to speed up computation (i.e. by avoiding for loops)\n",
    "    def multirange(self, counts):\n",
    "        counts = np.asarray(counts)\n",
    "        # Remove the following line if counts is always strictly positive.\n",
    "        counts = counts[counts != 0]\n",
    "        counts1 = counts[:-1]\n",
    "        reset_index = np.cumsum(counts1)\n",
    "\n",
    "        incr = np.ones(counts.sum(), dtype=int)\n",
    "        incr[0] = 0\n",
    "        incr[reset_index] = 1 - counts1\n",
    "\n",
    "        # Reuse the incr array for the final result.\n",
    "        incr.cumsum(out=incr)\n",
    "        return np.tile(incr, (self.K,1)).T        \n",
    "    \n",
    "    # DDMM implementation\n",
    "    def __init__(self, K, T, vocab, bows, alpha = None, beta = None):\n",
    "        \n",
    "        self.K = K\n",
    "        self.vocab = vocab\n",
    "        self.T = T\n",
    "        self.bows = bows\n",
    "        if alpha == None:\n",
    "            self.alpha = 1.3\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        if beta == None:\n",
    "            self.beta = 0.02\n",
    "        else:\n",
    "            self.beta = beta\n",
    "        \n",
    "        # create cluster assignment for each time step\n",
    "        self.clust_asgn = []\n",
    "        \n",
    "        # create number of epochs to convergence for each time step\n",
    "        self.epoch_conv = []\n",
    "        \n",
    "        # create epoch similarity for each time step\n",
    "        self.epoch_similarity = []\n",
    "        \n",
    "        # create alphas\n",
    "        self.alphas = []\n",
    "        \n",
    "        # create betas\n",
    "        self.betas = []\n",
    "        \n",
    "        # create number of large enough clusters\n",
    "        self.n_large_clusts = []\n",
    "        \n",
    "    def fit(self, max_epochs = 5, conv_crit = 0.97, lbda = 1, mu = 1):\n",
    "        \n",
    "        #initialize vocab length and hyperparameters\n",
    "        V = len(self.vocab)\n",
    "        alpha_ = self.alpha * np.ones(self.K)\n",
    "        beta_ = self.beta * np.ones((self.K, V))\n",
    "        \n",
    "        #iterate through each timestep\n",
    "        for t in range(self.T):\n",
    "            \n",
    "            n_large_clust = [self.K]\n",
    "            \n",
    "            alpha_copy = copy.deepcopy(alpha_)\n",
    "            beta_copy = copy.deepcopy(beta_)\n",
    "            self.alphas.append(alpha_copy) #update alphas\n",
    "            self.betas.append(beta_copy) #update beta\n",
    "            \n",
    "            bow = self.bows[t] #get bow for time t\n",
    "            bow_np = np.array(bow) #convert to numpy array\n",
    "            D = bow.shape[0] #get document size\n",
    "            clusters = np.random.choice(self.K, D) #cluster initialization\n",
    "            \n",
    "            #compute mk, nk and nkv for each k,v\n",
    "            mk = np.bincount(clusters, minlength = self.K)\n",
    "            nk = np.bincount(clusters, weights = np.sum(bow, axis = 1), minlength = self.K).astype(int)\n",
    "            nkv = np.apply_along_axis(lambda x: np.bincount(clusters, weights = x, \n",
    "                                                            minlength = self.K), axis = 0, arr = bow)\n",
    "            \n",
    "            #number of words per document\n",
    "            Nd = np.sum(bow, axis = 1)\n",
    "            \n",
    "            #initialize number of epochs and within-epoch similarity (to compare with conv_crit)\n",
    "            n_epoch = 0\n",
    "            epoch_sim = 0\n",
    "            \n",
    "            #repeat until convergence or maximum epochs\n",
    "            while n_epoch < max_epochs and epoch_sim < conv_crit:\n",
    "                print('Starting epoch', n_epoch + 1, 'for time step', t)\n",
    "                curr_clust = copy.deepcopy(clusters) #take note of current cluster\n",
    "                \n",
    "                for d in range(D):\n",
    "                    \n",
    "                    #knock out d from its cluster; let say z is d's current cluster\n",
    "                    z = clusters[d] #get d's current cluster\n",
    "                    #modify mk, nk and nkv\n",
    "                    mk[z] -= 1\n",
    "                    nk[z] -= Nd[d]\n",
    "                    nkv[z] -= bow_np[d]\n",
    "                    \n",
    "                    #sample a new cluster for d from Equation 15 (use log form)\n",
    "                    #ignore sum(alpha_k) + D - 1 since it does not depend on k\n",
    "\n",
    "                    #first component: log of alpha_k + m_k\n",
    "                    first = np.log(alpha_ + mk)\n",
    "\n",
    "                    #second component: sum from v=1 to V; j = 1 to Ndv of log(beta_kv + n_kv + j - 1)\n",
    "                    temp = (beta_ + nkv).T\n",
    "                    rep = bow_np[d]\n",
    "                    temp1 = np.vstack(temp)\n",
    "                    temp1 = temp1[np.repeat(np.arange(temp1.shape[0]), rep)]\n",
    "                    temp2 = self.multirange(rep)\n",
    "                    second = np.sum(np.log(temp1 + temp2), axis = 0)\n",
    "\n",
    "                    #third component: sum from i=1 to Nd of log(sum of beta_kv for each v + n_k + i - 1)\n",
    "                    Ndd = Nd[d]\n",
    "                    temp = np.sum(beta_, axis = 1) + nk\n",
    "                    third = np.sum(np.log(np.tile(temp, (Ndd, 1)) + \\\n",
    "                                          np.tile(np.arange(0, Ndd, 1), (self.K, 1)).T), axis = 0)\n",
    "\n",
    "                    #sample a cluster from the proportions; let the cluster be q\n",
    "                    prop = softmax(first + second - third)\n",
    "                    q = np.random.choice(np.arange(self.K), p = prop) #new cluster assignment for d\n",
    "\n",
    "                    #update mq, nq and nqv\n",
    "                    mk[q] += 1\n",
    "                    nk[q] += Nd[d]\n",
    "                    nkv[q] += bow_np[d]\n",
    "\n",
    "                    #update cluster assignment\n",
    "                    clusters[d] = q\n",
    "                \n",
    "                n_epoch += 1 #update number of epoch\n",
    "                epoch_sim = np.sum(curr_clust == clusters)/D #update cluster similarity for this epoch\n",
    "                \n",
    "                clust_prop = np.bincount(clusters, minlength = self.K)/D\n",
    "                n_large_clust.append(len(clust_prop[clust_prop > 0.01]))\n",
    "                \n",
    "            #update tracker variables\n",
    "            self.clust_asgn.append(clusters)\n",
    "            self.epoch_conv.append(n_epoch)\n",
    "            self.epoch_similarity.append(epoch_sim)\n",
    "            \n",
    "            #update alpha and beta\n",
    "            \n",
    "            #compute mk, nk and nkv for each k,v\n",
    "            mk = np.bincount(clusters, minlength = self.K)\n",
    "            nk = np.bincount(clusters, weights = np.sum(bow, axis = 1), minlength = self.K).astype(int)\n",
    "            nkv = np.apply_along_axis(lambda x: np.bincount(clusters, weights = x, \n",
    "                                                            minlength = self.K), axis = 0, arr = bow)\n",
    "            \n",
    "            #number of words per document\n",
    "            Nd = np.sum(bow, axis = 1)\n",
    "            \n",
    "            #modify alpha and beta\n",
    "            alpha_ += lbda * mk\n",
    "            beta_ += mu * nkv\n",
    "            \n",
    "            #update number of large clusters\n",
    "            self.n_large_clusts.append(n_large_clust)\n",
    "\n",
    "    def cluster_size(self):\n",
    "        return [np.bincount(x, minlength = self.K) for x in self.clust_asgn]\n",
    "    \n",
    "    def top_words(self, n = 10):\n",
    "        result = {}\n",
    "        for t in range(self.T):\n",
    "            bow = self.bows[t]\n",
    "            clusters = self.clust_asgn[t]\n",
    "            result_t = []\n",
    "            for k in range(self.K):\n",
    "                result_t.append(np.array(self.vocab)[np.array(bow.iloc[np.where(clusters == k)].sum(axis = 0) \\\n",
    "                                                    .sort_values()[::-1].index[:n]).astype(int)])\n",
    "            result[t] = result_t\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check ODDMM Topic Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read trained ODDMM\n",
    "with open('title_tags_description_oddmm_k16_run1', 'rb') as model:\n",
    "    ddmm = pickle.load(model)\n",
    "\n",
    "# Check top words\n",
    "ddmm_top_words = ddmm.top_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect topic words for cluster k\n",
    "for k in range(16):\n",
    "    pd.DataFrame([ddmm_top_words[i][k] for i in range(0,8)]).T.to_csv('title_tags_description_oddmm_cluster' + str(k) + '_top10words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check DTM Topic Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DTM\n",
    "dtm = LdaSeqModel.load('title_tags_description_ldaseq_k16_run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read index to word json file\n",
    "### Warning: the keys (word indices) are string type\n",
    "with open('title_tags_description/title_tags_description_indexword.json') as json_file:\n",
    "    indexword_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect topic words for cluster k\n",
    "for k in range(16):\n",
    "    timesteps = 8\n",
    "    top_count = 10\n",
    "    topic_words = dtm.print_topic_times(k, top_terms=top_count)\n",
    "    across_timesteps = []\n",
    "    for i in range(timesteps):\n",
    "        within_timesteps = []\n",
    "        for j in range(top_count):\n",
    "            within_timesteps.append(indexword_dict[topic_words[i][j][0]])\n",
    "        across_timesteps.append(within_timesteps)\n",
    "    #pd.DataFrame(across_timesteps).T.to_csv('title_tags_description_dtm_cluster' + str(k) + '_top10words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(across_timesteps).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AMI for DDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "with open('title_tags_description' + '/' + 'title_tags_description' + '_ddmm_' + str(16) + '.pkl', 'rb') as model:\n",
    "    ddmm = pickle.load(model)\n",
    "    \n",
    "# Read meta file\n",
    "meta_genres = pd.read_csv('title_tags_description/title_tags_description_meta.csv', \\\n",
    "                   lineterminator='^', dtype={'genre':'category'}).genre.cat.codes\n",
    "\n",
    "calculate_ami(ddmm.clust_asgn, meta_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AMI for DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "ldaseq = LdaSeqModel.load('title_ldaseq_k16')\n",
    "\n",
    "# Read BoW file\n",
    "bow = np.load('title/title_bow.npy')\n",
    "\n",
    "# Read index to word json file\n",
    "### Warning: the keys (word indices) are string type\n",
    "with open('title/title_indexword.json') as json_file:\n",
    "    indexword_dict = json.load(json_file)\n",
    "    \n",
    "# Read meta file\n",
    "meta_genres = pd.read_csv('title/title_meta.csv', \\\n",
    "                   lineterminator='^', dtype={'genre':'category'}).genre.cat.codes\n",
    "\n",
    "# Create corpus format from bow for ldaseq\n",
    "corpus = []\n",
    "for i in range(len(bow)):\n",
    "    doc_corpus = []\n",
    "    for ind in list(np.where(bow[i]>0)[0]):\n",
    "        doc_corpus.append((ind, bow[i][ind]))\n",
    "    corpus.append(doc_corpus)\n",
    "    \n",
    "# Function to get ldaseq assigned clusters for documents\n",
    "def get_ldaseq_clusters(ldaseq, corpus):\n",
    "    clusters_list = []\n",
    "    for i in range(len(corpus)):\n",
    "        clusters_list.append(np.argmax(ldaseq.doc_topics(i)))\n",
    "    return clusters_list\n",
    "\n",
    "# Get ldaseq assigned clusters\n",
    "clusters = get_ldaseq_clusters(ldaseq, corpus)\n",
    "\n",
    "# Hack to make the AMI function work for ldaseq (since the AMI function requires multi-dim array)\n",
    "clusters = np.array(clusters).reshape(2, -1)\n",
    "\n",
    "def calculate_ami(clust_asgn, true_genres):\n",
    "    pred_clusters = np.concatenate(clust_asgn).ravel()\n",
    "    return adjusted_mutual_info_score(true_genres, pred_clusters)\n",
    "\n",
    "calculate_ami(clusters, meta_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph for cluster convergence/similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions_list = ['title',\n",
    "                 'title_tags',\n",
    "                 'title_tags_description']\n",
    "markers = ['o', 'v', '^']\n",
    "K = 16\n",
    "\n",
    "plt.figure()\n",
    "    \n",
    "for i, version_name in enumerate(versions_list):\n",
    "\n",
    "    with open(version_name + '_oddmm_' + 'k16_run1.pkl', 'rb') as model:\n",
    "        ddmm = pickle.load(model)\n",
    "\n",
    "    temp = ddmm.epoch_similarity\n",
    "    plt.plot(np.arange(len(temp)), temp, marker = markers[i], label = version_name);\n",
    "\n",
    "plt.legend(loc=(1.04,0.4));\n",
    "plt.yticks(np.arange(0.84,1,0.02));\n",
    "plt.xlabel('Time step (t)');\n",
    "plt.ylabel('Cluster similarity (4th and 5th epochs)');\n",
    "plt.figtext(0.95, 0.3, 'Algorithm is run for 5 epochs');\n",
    "plt.title('Cluster similarity for each time step (K = ' + str(K) + ')');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph for AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions_list = ['title', 'tags', 'description', \n",
    "                 'title_tags', 'title_description', \n",
    "                 'tags_description', 'title_tags_description']\n",
    "markers = ['o', 'v', '^', '<', '>', 's', 'X']\n",
    "K_list = [8,16,32]\n",
    "\n",
    "version_ami_lists = []\n",
    "for version_name in version_list:\n",
    "    temp = []\n",
    "    meta_genres = pd.read_csv(version_name + '/' + version_name + '_meta.csv', lineterminator='^', \\\n",
    "                      dtype={'genre':'category'}).genre.cat.codes\n",
    "    for K in K_list:\n",
    "        temp2 = []\n",
    "        for run in range(1,6):\n",
    "            with open(version_name + '_oddmm_' + 'k' + str(K) + '_run' + str(run) + '.pkl', 'rb') as model:\n",
    "                ddmm = pickle.load(model)\n",
    "            temp2.append(calculate_ami(ddmm.clust_asgn, meta_genres))\n",
    "        temp.append(np.mean(temp2))\n",
    "    version_ami_lists.append(temp)\n",
    "\n",
    "for i, version_name in enumerate(versions_list):\n",
    "    version_ami_list = version_ami_lists[i]\n",
    "    plt.plot([0,1,2], version_ami_list, label = version_name, marker = markers[i]);\n",
    "        \n",
    "plt.xticks([0,1,2], K_list)\n",
    "plt.legend(loc=(1.04, 0.48));\n",
    "plt.xlabel('Number of clusters (K)');\n",
    "plt.ylabel('Adjusted mutual information (AMI)');\n",
    "plt.figtext(0.95, 0.38, 'AMI scores are averaged over 5 runs');\n",
    "plt.title('Adjusted mutual information (AMI) with different number of clusters');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
