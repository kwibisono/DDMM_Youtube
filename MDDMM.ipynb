{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define DDMM model class\n",
    "\n",
    "class DDMM():\n",
    "    \n",
    "    #Miscellaneous function to speed up computation (i.e. by avoiding for loops)\n",
    "    def multirange(self, counts):\n",
    "        counts = np.asarray(counts)\n",
    "        # Remove the following line if counts is always strictly positive.\n",
    "        counts = counts[counts != 0]\n",
    "        counts1 = counts[:-1]\n",
    "        reset_index = np.cumsum(counts1)\n",
    "\n",
    "        incr = np.ones(counts.sum(), dtype=int)\n",
    "        incr[0] = 0\n",
    "        incr[reset_index] = 1 - counts1\n",
    "\n",
    "        # Reuse the incr array for the final result.\n",
    "        incr.cumsum(out=incr)\n",
    "        return np.tile(incr, (self.K,1)).T        \n",
    "    \n",
    "    # DDMM implementation\n",
    "    def __init__(self, K, T, vocab, bows, alpha = None, beta = None):\n",
    "        \n",
    "        self.K = K\n",
    "        self.vocab = vocab\n",
    "        self.T = T\n",
    "        self.bows = bows\n",
    "        if alpha == None:\n",
    "            self.alpha = 1.3\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        if beta == None:\n",
    "            self.beta = 0.02\n",
    "        else:\n",
    "            self.beta = beta\n",
    "        \n",
    "        # create cluster assignment for each time step\n",
    "        self.clust_asgn = []\n",
    "        \n",
    "        # create number of epochs to convergence for each time step\n",
    "        self.epoch_conv = []\n",
    "        \n",
    "        # create epoch similarity for each time step\n",
    "        self.epoch_similarity = []\n",
    "        \n",
    "        # create alphas\n",
    "        self.alphas = []\n",
    "        \n",
    "        # create betas\n",
    "        self.betas = []\n",
    "        \n",
    "        # create number of large enough clusters\n",
    "        self.n_large_clusts = []\n",
    "        \n",
    "    def fit(self, max_epochs = 5, conv_crit = 0.97, lbda = 1, mu = 1):\n",
    "        \n",
    "        #initialize vocab length and hyperparameters\n",
    "        V = len(self.vocab)\n",
    "        alpha_ = self.alpha * np.ones(self.K)\n",
    "        beta_ = self.beta * np.ones((self.K, V))\n",
    "        \n",
    "        #iterate through each timestep\n",
    "        for t in range(self.T):\n",
    "            \n",
    "            n_large_clust = [self.K]\n",
    "            \n",
    "            alpha_copy = copy.deepcopy(alpha_)\n",
    "            beta_copy = copy.deepcopy(beta_)\n",
    "            self.alphas.append(alpha_copy) #update alphas\n",
    "            self.betas.append(beta_copy) #update beta\n",
    "            \n",
    "            bow = self.bows[t] #get bow for time t\n",
    "            bow_np = np.array(bow) #convert to numpy array\n",
    "            D = bow.shape[0] #get document size\n",
    "            clusters = np.random.choice(self.K, D) #cluster initialization\n",
    "            \n",
    "            #compute mk, nk and nkv for each k,v\n",
    "            mk = np.bincount(clusters, minlength = self.K)\n",
    "            nk = np.bincount(clusters, weights = np.sum(bow, axis = 1), minlength = self.K).astype(int)\n",
    "            nkv = np.apply_along_axis(lambda x: np.bincount(clusters, weights = x, \n",
    "                                                            minlength = self.K), axis = 0, arr = bow)\n",
    "            \n",
    "            #number of words per document\n",
    "            Nd = np.sum(bow, axis = 1)\n",
    "            \n",
    "            #initialize number of epochs and within-epoch similarity (to compare with conv_crit)\n",
    "            n_epoch = 0\n",
    "            epoch_sim = 0\n",
    "            \n",
    "            #repeat until convergence or maximum epochs\n",
    "            while n_epoch < max_epochs and epoch_sim < conv_crit:\n",
    "                print('Starting epoch', n_epoch + 1, 'for time step', t)\n",
    "                curr_clust = copy.deepcopy(clusters) #take note of current cluster\n",
    "                \n",
    "                for d in range(D):\n",
    "                    \n",
    "                    #knock out d from its cluster; let say z is d's current cluster\n",
    "                    z = clusters[d] #get d's current cluster\n",
    "                    #modify mk, nk and nkv\n",
    "                    mk[z] -= 1\n",
    "                    nk[z] -= Nd[d]\n",
    "                    nkv[z] -= bow_np[d]\n",
    "                    \n",
    "                    #sample a new cluster for d from Equation 15 (use log form)\n",
    "                    #ignore sum(alpha_k) + D - 1 since it does not depend on k\n",
    "\n",
    "                    #first component: log of alpha_k + m_k\n",
    "                    first = np.log(alpha_ + mk)\n",
    "\n",
    "                    #second component: sum from v=1 to V; j = 1 to Ndv of log(beta_kv + n_kv + j - 1)\n",
    "                    temp = (beta_ + nkv).T\n",
    "                    rep = bow_np[d]\n",
    "                    temp1 = np.vstack(temp)\n",
    "                    temp1 = temp1[np.repeat(np.arange(temp1.shape[0]), rep)]\n",
    "                    temp2 = self.multirange(rep)\n",
    "                    second = np.sum(np.log(temp1 + temp2), axis = 0)\n",
    "\n",
    "                    #third component: sum from i=1 to Nd of log(sum of beta_kv for each v + n_k + i - 1)\n",
    "                    Ndd = Nd[d]\n",
    "                    temp = np.sum(beta_, axis = 1) + nk\n",
    "                    third = np.sum(np.log(np.tile(temp, (Ndd, 1)) + \\\n",
    "                                          np.tile(np.arange(0, Ndd, 1), (self.K, 1)).T), axis = 0)\n",
    "\n",
    "                    #sample a cluster from the proportions; let the cluster be q\n",
    "                    prop = softmax(first + second - third)\n",
    "                    q = np.random.choice(np.arange(self.K), p = prop) #new cluster assignment for d\n",
    "\n",
    "                    #update mq, nq and nqv\n",
    "                    mk[q] += 1\n",
    "                    nk[q] += Nd[d]\n",
    "                    nkv[q] += bow_np[d]\n",
    "\n",
    "                    #update cluster assignment\n",
    "                    clusters[d] = q\n",
    "                \n",
    "                n_epoch += 1 #update number of epoch\n",
    "                epoch_sim = np.sum(curr_clust == clusters)/D #update cluster similarity for this epoch\n",
    "                \n",
    "                clust_prop = np.bincount(clusters, minlength = self.K)/D\n",
    "                n_large_clust.append(len(clust_prop[clust_prop > 0.01]))\n",
    "                \n",
    "            #update tracker variables\n",
    "            self.clust_asgn.append(clusters)\n",
    "            self.epoch_conv.append(n_epoch)\n",
    "            self.epoch_similarity.append(epoch_sim)\n",
    "            \n",
    "            #update alpha and beta\n",
    "            \n",
    "            #compute mk, nk and nkv for each k,v\n",
    "            mk = np.bincount(clusters, minlength = self.K)\n",
    "            nk = np.bincount(clusters, weights = np.sum(bow, axis = 1), minlength = self.K).astype(int)\n",
    "            nkv = np.apply_along_axis(lambda x: np.bincount(clusters, weights = x, \n",
    "                                                            minlength = self.K), axis = 0, arr = bow)\n",
    "            \n",
    "            #number of words per document\n",
    "            Nd = np.sum(bow, axis = 1)\n",
    "            \n",
    "            #modify alpha and beta\n",
    "            alpha_ += lbda * mk\n",
    "            beta_ += mu * nkv\n",
    "            \n",
    "            #update number of large clusters\n",
    "            self.n_large_clusts.append(n_large_clust)\n",
    "\n",
    "    def cluster_size(self):\n",
    "        return [np.bincount(x, minlength = self.K) for x in self.clust_asgn]\n",
    "    \n",
    "    def top_words(self, n = 10):\n",
    "        result = {}\n",
    "        for t in range(self.T):\n",
    "            bow = self.bows[t]\n",
    "            clusters = self.clust_asgn[t]\n",
    "            result_t = []\n",
    "            for k in range(self.K):\n",
    "                result_t.append(np.array(self.vocab)[np.array(bow.iloc[np.where(clusters == k)].sum(axis = 0) \\\n",
    "                                                    .sort_values()[::-1].index[:n]).astype(int)])\n",
    "            result[t] = result_t\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_name = 'title_tags_description'\n",
    "print('Training for {}'.format(version_name))\n",
    "meta = pd.read_csv(version_name + '/' + version_name + '_meta.csv', lineterminator='^')\n",
    "\n",
    "# Read index to word json file\n",
    "### Warning: the keys (word indices) are string type\n",
    "with open(version_name + '/' + version_name + '_indexword.json') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "\n",
    "vocab = list(vocab.values())\n",
    "\n",
    "# Read BoW file\n",
    "bows = np.load(version_name + '/' + version_name + '_bow.npy')\n",
    "\n",
    "T = len(np.unique(meta['t_month']))\n",
    "\n",
    "bows = [pd.DataFrame(bows[meta[meta['t_month']==i].index]) for i in range(T)]\n",
    "\n",
    "ddmm = DDMM(K=16, T=T, vocab=vocab, bows=bows)\n",
    "ddmm.fit()\n",
    "with open(version_name + '/' + version_name + '_oddmm_' + str(16) + '.pkl', 'wb') as output:\n",
    "    pickle.dump(ddmm, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
